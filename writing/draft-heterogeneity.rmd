---
title: "Limited evidence for widespread heterogeneity in psychology"
author: "Anton Ohlsson Collentine"
header-includes:
   - \setlength\parindent{24pt}
   - \usepackage{setspace}
   - \doublespacing
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r load data and packages}
library(readr)
library(dplyr)
library(kableExtra)
library(metafor)
library(purrr)

dat <- read_csv("../data/collated_summary_data.csv")
```


```{r prep-heterogeneity-table}
#library(metafor)
#library(dplyr)
#library(purrr)

est_heterogen_smd_raw <- function(x){
  if(any(x[, "effect_type"] == "Risk difference")){ #without the 'any' we will get warnings because we apply 'if' to a vector
    
    fit <- rma(measure = "RD", ai = outcome_t1, bi = outcome_t2, ci = outcome_c1, di = outcome_c2,  n1i = ntreatment, n2i = ncontrol,  data = x)
    
  } else if(any(x[, "outcomes1_2"] == "mean _ SE")){  
    
    fit <-  rma(yi = effect_size, sei = outcome_c2,  data = x) 
  
  } else if(any(x[, "effect_type"] == "Raw mean difference")){
    if(any(x[, "rs"] %in% c("RRR5", "RRR7"))){ #For these us the Knapp and Hartung adjustment of standard errors
      
      fit <- rma(measure = "MD", m1i = outcome_t1, m2i = outcome_c1, sd1i = outcome_t2, sd2i = outcome_c2, n1i = ntreatment, n2i = ncontrol, test ="knha", data = x)
    } else{
      fit <- rma(measure = "MD", m1i = outcome_t1, m2i = outcome_c1, sd1i = outcome_t2, sd2i = outcome_c2, n1i = ntreatment, n2i = ncontrol, data = x)
    }
  
  } else if(any(x[, "outcomes1_2"] == "mean _ SD")){  
    
    fit <- rma(measure = "SMD", m1i = outcome_t1, m2i = outcome_c1, sd1i = outcome_t2, sd2i = outcome_c2, n1i = ntreatment, n2i = ncontrol, data = x) 
       
  } else if(any(x[, "effect_type"] == "r")){
    if(any(x[, "rs"] == "ML1")){
      fit <- rma(measure = "COR", ri = effect_size, ni = Ntotal,  data = x)
    } else{ #for ML3
      fit <- rma(measure = "UCOR", ri = effect_size, ni = Ntotal,  data = x, vtype = "UB")
      #Note that ML3 specifies measure = "COR" with vtype = "UB", but that this is specified as above in the latest metafor 
    }

  } else{ #for all two-group count effects
    
    fit <- rma(measure = "OR2DL", ai = outcome_t1, bi = outcome_t2, ci = outcome_c1, di = outcome_c2, 
               n1i = ntreatment, n2i = ncontrol,  data = x)
    #standardized, ML1 used 'OR2D' which = 'OR2DL' in latest version of metafor
    
  }
  
  I2_1 <- confint(fit)$random[3, ] #Gives us the I2 estimate and its confidence interval
  data.frame(eff_size = fit$b[[1]], #effect size (point estimate)
             s_I2 = I2_1[1], s_ci.lb = I2_1[2], s_ci.ub = I2_1[3]) #I2 + CI
}

#Apply function to data
res <- dat %>% 
  split(.$effect) %>%  #separate by effect, necessary step otherwise the function is applied overall
  map_dfr(est_heterogen_smd_raw, .id = "effect") #apply function, rbind results into dataframe ('dfr'), make sure purrr is up to date

OR <- c("Allowed vs. forbidden", "Gain vs. loss framing", "Norm of reciprocity", "Low vs. high category scales") #Effects reported as d in many labs 1 but meta-analyzed as OR

#Add rs to the data-frame
het <- dat %>% 
  select(rs, Site, effect, effect_type) %>% 
  group_by(rs, effect) %>% 
  summarize(effect_type = unique(effect_type),
            k = n_distinct(Site)) %>% 
  left_join(res) %>% 
  mutate(effect_type = recode(effect_type, 
                              d = "SMD.",
                              'Raw mean difference' = "MD",
                              'Risk difference' = "RD")) %>% 
  select(rs, effect,k , everything()) %>% 
  arrange(desc(s_I2),  desc(s_ci.ub)) #sort by I2 and upper CI bound
```
Empirical research is typically portrayed as proceeding in two stages. First, belief in the existence of an effect is established. Second, its generalizability is examined by exploring its boundary conditions. In the first stage, inferential statistics are used  to minimize the risk that a discovery is due to sampling error.  In the second stage, one may ask to what extent the effect depends on a particular choice of four contextual factors; the 1) sample population, 2) settings, 3) treatment variables and 4) measurement variables [ref: Campbell & Stanley]. This extent is often explored through replications of the original study that are either as similar as possible to the original (called 'direct' or 'exact' replications) or with some deliberate variation on conceptual factors (so-called 'conceptual' or 'indirect' replications) [ref: Zwaan et al], and once sufficient studies have accumulated through meta-analysis. In meta-analysis, the heterogeneity of an effect size (henceforth referred to as heterogeneity) is a measure of an effect's susceptibility to changes in these four factors. An effect strongly dependent on one or more of the four factors, unless controlled for, should exhibit high heterogeneity. In this paper we examine what degree of heterogeneity can be expected in replication studies in psychology and explore a proposed relationship with effect size. We focus in this paper on direct replications.

Heterogeneity is of concern for several reasons. First and foremost, unaccounted for heterogeneity can have unacceptable consequences. This may be most readily evident for medicine, where in the case of heterogeneity an intervention that is successful for some may have direct negative health consequences for others. However, the adoption of psychological research for business, law enforcement, and policymaking in general (e.g., the widespread use of 'nudges'; https://www.knowablemagazine.org/article/society/2018/nudging-grows-and-now-has-government-job) means its impact can be widespread and the consequences of heterogeneity should be no less of a concern for psychologists.

 Second, unaccounted for heterogeneity is an indication of incomplete theory, since it suggests that a theory is unable to predict all contextual factors of importance to its claims. As such, heterogeneity might imply previously unknown predictors, so called 'hidden moderators' [ref: van Bavel], the discovery of which can be seen as an opportunity for theoretical advancement [ref: Tacket et al]. The corollary is that, unless a theory has predefined additional assumptions ('auxiliary assumptions' [ref: Earp & Trafimow]) which were not fulfilled in the replications, an absence of heterogeneity should generally preclude referral to hidden moderators. 
 
 Third,  the possibility of heterogeneity can create controversy in the interpretation of single replication results. The proclamation of a  'failure' to replicate an effect often seems to imply that the original find was merely a statistical fluke, due to 'p-hacking' [Ref: Simmons et al] or publication bias [ref.. Inzlicht?]. Unsurprisingly, some researchers take offense [e.g., ref: Baumeister]. An alternative explanation for non-replication, often espoused by the original authors [e.g, IJzerman; Strack], is that the effect is more heterogenous than (perhaps implicitly) claimed originally. Such explanations may be valid or not, but even if valid, an effect is typically of less general interest the more specific circumstances it requires to appear. A way for original authors to align researchers' expectations of heterogeneity is to preemptively specify their own, that is, the degree of heterogeneity that would make they themselves lose interest in an effect (e.g., by declaring 'constraints on generality' [ref Simons et al].

Fourth, meta-analytic techniques are affected by heterogeneity. Techniques that attempt to correct for publication bias in their estimate tend to fail in the presence of heterogeneity [refs: Inzlicht; McShane], which is problematic considering widespread publication bias [refs] and its impact on meta-analytic estimates. In addition, heterogeneity alters the interpretation of meta-analytic estimates as either _the_ true effect size (under homogeneity) or the average of the true effect sizes (under heterogeneity), though one may question the usefulness of interpreting the average true effect size in the presence of heterogeneity [ref: blog Simonsohn], just as it may be questionable to interpret an average main effect in the context of an interaction effect [ref Aiken and West 1993].

It is a commonly believed that heterogeneity is the norm in psychology.  In support of this notion, recent large scale reviews of meta-analyses in psychology [ref: Stanley; van Erp et al] report median heterogeneity levels that can best be described as 'large' (see next paragraph). In comparison, the median heterogeneity estimate in medicine would be considered 'small' by the same standard [ref: Ioannidis  "uncertainty in heterogeneity"]. It may simply be that effects in psychology are more heterogenous than those of medicine. However, meta-analyses in psychology also typically include more studies than those in medicine, and it could be that they tend to include studies from a much broader spectrum than meta-analyses in medicine. The median number of studies (effects) per meta-analysis in the psychology sample of [van Erp et al] was 12, whereas in the medicine sample of  [Ioannidis; uncertainty in heterogeneity] the median number of studies was only 7, despite exclusion of all meta-analyses of 3 or fewer studies. It is difficult to separate these explanations. To facilitate doing so, in this paper we focus on meta-analyses of only direct replications, which are exempt from the potential problem of including too disparate studies. In reference to meta-analyses of direct replications, several authors [ref: McShane et al; Tacket et al] have argued that if we were to expect heterogeneity to be absent or minimal anywhere, it would be in pre-registered multi-lab projects with a common protocol, and that heterogeneity has been reported even under such circumstances [e.g., ref: Many Labs 1] is an indication of widespread heterogeneity in psychology. However, even in the case of multi-lab direct replication projects, studies vary on two contextual factors (sample population and settings) and if we believe an effect is sensitive to changes in these two factors we might still expect to find some heterogeneity

One problem lies in the assessment of heterogeneity and its inherent uncertainty. Heterogeneity is often measured by the I2 index [ref: Higgins & Higgins], which allows comparison of estimates across meta-analyses and has an intuitive interpretation. It can be interpreted as the percentage of variability in effect sizes that is due to heterogeneity amongst the true effects (that is, sensitivity to contextual factors) rather than sampling variance, and ranges from 0-100%. The I2 index is a transformation of the well-known Q-test of heterogeneity in meta-analysis [ref: Cochran] by taking $[Q - (k - 1)] / Q \times 100$%, where k is the number of studies included in the meta-analysis and a negative I2 is set to zero.  [Higgins et al] tentatively defined I2-values of 25, 50, and 75% as small/medium/large heterogeneity respectively. Tests of heterogeneity typically have low statistical power in many practical situations [ref: Jackson; Huedo-Medina]. This complicates the discussion of heterogeneity, because while I2 always provides an estimate of heterogeneity, this estimate is often surrounded by wide confidence intervals [ref: Ioannidis  "uncertainty in heterogeneity"]. For example, Ioannidis reports that in a large set of Cochrane meta-analyses, all meta-analyses with I2 point estimates of 0% had upper 95% confidence intervals that exceeded 33%, exceeding what [Higgins] defined as 'small' heterogeneity. In addition, under homogeneity I2 has a central chisquare distribution [ref: von Hippel], a distribution that is right-skewed with more than 40% of observations falling above the expected value (for all k > 4). In other words, even in the absence of heterogeneity, a meta-analysis of 5 or more studies will have an I2 point estimate above zero in more than 40% of cases. Heterogeneity estimates may thus be congruent with a wide range of true heterogeneity levels. Despite exhortations to the contrary [ref: Ioannidis "uncertainty in heterogeneity"] , it remains common to omit confidence intervals in the reporting of I2. In consideration of such uncertainty and the prevalent belief that heterogeneity is the norm in psychology, we examine the existing evidence for heterogeneity in psychology using a sample of pre-registered multi-lab direct replication projects.

Effect size likely influences heterogeneity. Intuitively, it makes sense to believe that if there is no meta-analytic effect there is nothing to moderate (i.e., no heterogeneity). However, a null or near null effect size estimate may arise from failure to consider contextual factors ('hidden moderators', [ref: van Bavel]) and does not by itself imply the absence of heterogeneity. A large meta-analytic effect size on the other hand, can be expected to be associated with more heterogeneity. To illustrate, consider a meta-analysis of say, the correlation between neuroticism and procrastination. Each included study would need to measure the two variables somehow, possibly the same way across studies. However, because of individual differences, measurement is not equally exact for everyone, and between studies the measurement reliabilities may differ either due to sampling variance (that the sample happens to be more or less homogenous) or to differences in contextual factors (e.g., sampling population, method of measurement). This means that even if the underlying true effect size is the same, the observed correlation between the two variables will differ between studies. Keeping measurement reliabilities constant, differences in observed effect sizes will increase with the underlying true effect, resulting in more variability being ascribed to heterogeneity. More formally, an observed correlation $r_{xy}$ can be expressed as the product of the true correlation or effect size, $\rho_{xy}$, multiplied by the square root of the measurement reliabilities for X ($R_{xx'}$) and Y ($R_{yy'}$): $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$. As such, if $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ stays constant while $\rho_{xy}$ increases the observed differences between studies will also increase, thereby increasing heterogeneity of observed effect sizes (see Table 1). We explore the association between effect size and heterogeneity in a sample of pre-registered multi-lab replication projects.

##Table 1.
###Heterogeneity may increase with effect size
```{r effect-size-heterogeneity-table}
#library(kableExtra)
rooted_relabilities <- matrix(rep(c(.8, .7,.6), 3), ncol = 3)
rho <- c(0, .5, .8)
rel <- c("R = .8", "R = .7", "R = .6")

rooted_relabilities <- sweep(rooted_relabilities, 2, rho, "*")
diff_row <- data.frame(rel = "Diff.", X1 = 0, X2 = 0.05, X3 = 0.08)

het_demo <- data.frame(rel, rooted_relabilities)
het_demo <- rbind(het_demo, diff_row)

het_demo %>% knitr::kable("latex", booktabs = T, col.names = c("", "Rho = 0", "Rho = .5", "Rho = .8")) %>% 
  kable_styling() %>% 
  footnote(threeparttable = TRUE,
           general = "R = square root of the multiplied reliabilities") %>% 
  row_spec(3, hline_after = T)

```

We examine the evidence for widespread heterogeneity in psychology and explore the correlation between effect size and heterogeneity, using 10 pre-registered multi-lab replication projects in psychology (Table 2). These 10 projects examined a total of 37 primary outcome variables and arguably represent the best, least biased, meta-analytic data currently available in psychology. To better interpret heterogeneity estimates we also estimate power of each project to find zero/small/medium/large heterogeneity as defined by [Higgins et al]. Of the four contextual factors a study might vary on (sample population, settings,  treatment variables, and measurement variables) the studies in these projects vary only on two: sample population and settings. Nonetheless, if we believe an effect is sensitive to changes in these two factors we may still expect to find some heterogeneity. For example, the variety of countries and samples involved in some of the replication projects, not all of which are WEIRD (Western, Educated, Industrial, Rich, Democratic [ref: Weird]) might be a reason to expect heterogeneity. On the other hand, it may be that the less disparate collection of studies included in the these direct replication meta-analyses as compared to most psychological meta-analyses result in low heterogeneity estimates, similar to estimates in medicine. Regardless of outcome, the sampled multi-lab replication projects can only inform us of susceptibility to changes in the varied contextual factors  (sample population and settings), with no information on susceptibility to changes in other contextual factors.

##Table 2.
```{r data-summary-table}
#library(dplyr)
#library(kableExtra)

#Shortened version of APA reference
papers <- c('Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Jr., Bahník, S., Bernstein, M. J., . . . Nosek, B. A. (2014). Investigating variation in replicability: A "many labs" replication project.', 
'Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., ... & Brown, E. R. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication.',
'Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., ... & Brown, E. R. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication.',
'Alogna, V. K., Attaya, M. K., Aucoin, P., Bahník, S., Birch, S., Birt, A. R., ... & Buswell, K. (2014). Registered replication report: Schooler and engstler-schooler (1990).',
'Eerland, A., Sherrill, A. M., Magliano, J. P., Zwaan, R. A., Arnal, J. D., Aucoin, P., ... & Crocker, C. (2016). Registered replication report: Hart & Albarracín (2011).',
'Hagger, M. S., Chatzisarantis, N. L., Alberts, H., Anggono, C. O., Batailler, C., Birt, A. R., ... & Calvillo, D. P. (2016). A multilab preregistered replication of the ego-depletion effect.',
'Cheung, I., Campbell, L., LeBel, E. P., Ackerman, R. A., Aykutoglu, B., Bahník, S., ... & Carcedo, R. J. (2016). Registered Replication Report: Study 1 from Finkel, Rusbult, Kumashiro, & Hannon (2002).',
'Wagenmakers, E. J., Beek, T., Dijkhoff, L., Gronau, Q. F., Acosta, A., Adams Jr, R. B., ... & Bulnes, L. C. (2016). Registered Replication Report: Strack, Martin, & Stepper (1988).',
'Bouwmeester, S., Verkoeijen, P. P., Aczel, B., Barbosa, F., Bègue, L., Brañas-Garza, P., ... & Evans, A. M. (2017). Registered Replication Report: Rand, Greene, and Nowak (2012).',
"O'Donnell, M., Nelson, L., McLatchie, N. M., & Lynott, D. J. (2017). Registered Replication Report: Dijksterhuis & van Knippenberg (1998)")

summary.table <- dat %>% 
  group_by(rs) %>% 
  summarize(k = n_distinct(Site),
            Countries = n_distinct(country), 
            Effects = n_distinct(effect),
            Participants = if(Effects == 1){sum(Ntotal)}
                           else{sum(Ntotal) / Effects}) %>% #average across effects
  mutate(Paper = papers) %>% 
  select(RS = rs, Paper, everything()) #change order of columns

summary.table %>% knitr::kable("latex", booktabs = T, digits = 0) %>% 
  kable_styling() %>% 
  column_spec(2, width = "25em") %>% #specify width of paper column
  footnote(threeparttable = TRUE,
           general = "For studies with several effects the number of participants is the average across effects, rounded to the closest whole number. Participant numbers are those used for primary analyses by original authors (i.e., after exclusions). RS = Replication Study, k = no. primary studies, ML = Many Labs, RRR = Registered Replication Report. (OSF.io/XXXX)")
```

###Method

All code and data for this project are available on the Open Science Framework (OSF) at osf.io/XXXX. We refer to relevant files on OSF using brackets and links in the sections below, that is (osf.io/XXXX/). We ran all analyses using R version X [ref: R citation].

We downloaded and collated summary data from 10 pre-registered multi-lab replication projects in psychology (Table 2). Data from all 10 projects were publicly available on the Open Science Framework (osf.io) and downloaded between 2018/02/01 and 2018/03/31. Although some projects (e.g. RRR4) reported results from several outcome variables, we only included primary outcome variables as stated in accompanying publications, resulting in a total of 37 effects. For each effect we extracted (osf.io/XXXX/) summary data (e.g., means and standard deviations) at the level of the lab as specified by the original authors for their primary analysis (i.e., typically after exclusions). We extracted information on the country of each lab, whether participants were physically in the lab for the study, total number of participants per lab, type of effect size, and additional information related to each effect (see codebook; osf.io/XXXX/). Extracted data were in a variety of formats: Excel (Many labs 1, RRR1 & RRR2), CSV (Many labs 3, RRR3, RRR4, RRR5, RRR6) and as PDF tables (RRR7, RRR8). In two cases (RRR5 and RRR6) it was necessary to download the raw data to extract summary data. Although a particular lab may have participated in several projects, the lab indicator was typically not the same across projects. Even so, we kept the original lab indicators to facilitate comparing observations in our dataset with the original datasets. Finally, we collated the summary data for all effects into one dataset for analysis (osf.io/XXXX/). 

To examine heterogeneity across effects, we computed meta-analytic estimates for all 37 effects in our dataset (Table 3). We ran all analyses as specified by the replication authors (osf.io/XXXX/). All effects were estimated with random-effects models and the Restricted Maximum Likelihood (REML) estimator using the R-package metafor [ref], though with a variety of outcome variables: correlations, standardized mean differences, raw mean differences, odds ratios and risk differences. ML1 transformed odd ratios into standardized mean differences when meta-analyzing under the assumption that responses followed logistic distributions [ref: Sanchez-Meca; metafor]. Two projects (RRR5 and RRR7) used the Knapp and Hartung adjustment of the standard errors [ref: Knapp & Hartung] and ML3 correlations were corrected for bias [ref: Hedges 1989; metafor]. After estimating effects, ML3 transformed correlations into eta-squared for reporting, which we did not. For each effect we estimated I2 and its 95% confidence interval. Confidence intervals were calculated in metafor which uses the Q-profile method [ref Jackson 2014].

[Ignore the following paragraph - power simulation paragraph - subject to change]
To facilitate interpreting results, we estimated power and type 1 error for each of the 37 effects under zero/small/medium/large heterogeneity (I2 = 0/25/50/75%  respectively), as well as the expected I2-distributions across effects at these heterogeneity levels (osf.io/XXXX/). To do so we simulated results for each effect given the number of studies, k, and the sample sizes of each study, $N_k$, for that effect. Our approach consisted of (i) varying the between-studies variance,  $\tau^2$, for each effect to find the values that best corresponded to I2-estimates of 25/50/75% and (ii) using the found values to estimate power and type 1 error by computing the proportion non-significant results over a large number of replications. In step (i), we varied $\tau^2$ between 0.000 and 0.250 in increments of 0.005,  and replicated each condition 50 times.  We then computed the average I2 across the 50 replications, discarding non-converging models, and for each effect saved the $\tau^2$ values that best corresponded to small, medium, and large I2.  For homogeneity we set $\tau^2$ to zero. In step (ii), to estimate power and type 1 error, we ran one thousand replications at each identified level of heterogeneity using the $\tau^2$-values of step (i). We considered a result significant when the lower bound of the 95% I2 confidence interval was above zero and save heterogeneity estimates were saved from each replication for plotting I2-distributions.
 
[Ignore the following paragraph - power simulation paragraph - subject to change]
In both steps of our simulation, we followed the same data generating procedure. For each effect, we generated k effect sizes, $\theta_k$, from a normal distribution with a mean of zero and between-studies variance $\tau^2$: $\theta_k \sim N(0,  \tau^2)$.  For the sake of simplicity, we made the choice to set the average effect size to zero and treated each effect as consisting of a treatment and control group. We thus drew an effect size $\theta_k$ for each replication study of an effect, and within each study we then generated $N_k / 2$ (rounded to closest integer) sample values for a treatment and for a control group from two normal distributions with means $\mu_t = \theta_k$ and $\mu_c = 0$ and equal within-study variances ($\sigma^2 = 1$). This provided sample means and standard deviations for all k treatment and control groups of an effect, which we then used to estimate a random effects model for standardized mean differences in metafor using REML. This process was repeated the desired number of times for each condition and effect.

To examine the correlation between effect size and I2 we converted all effect sizes to a common metric (osf.io/XXXX/). Since I2 is logically and mathematically bounded at zero, we also correlated effect sizes with the closely related heterogeneity estimate H2, which is calculated as $H2 = Q  / (k - 1)$ and equals $1 / (1 - I2)$ for $I2 \geq 0$. We first converted all effects into correlations (r) and used the R-package metafor [ref] to estimate I2 and meta-analytic effects expressed as Fisher's z. In doing so, we fitted random-effects models with metafor's default REML estimator. Fisher's z is a normalizing correlation transformation that ranges from negative infinity to positive infinity, though except for extreme correlations it stays close to the -1 to 1 range [ref]. For mean differences we calculated the pooled standard deviation, [ref Borenstein in Cooper & Hedges, 2009, p. 226), standardized the effect size and converted it to a correlation (p. 234) with a correction factor for unequal sample sizes (p. 234). In one case (RRR8), we first had to convert reported standard errors into pooled standard deviation (p.224). For risk differences and odds ratios we first added 1/2 to a cell if it was empty to avoid dividing by zero, next calculated the logarithmic odds ratio (p. 266), converted this to Cohen's d (p. 232) and then finally to a correlation. All formulas used are presented in Appendix A. 

##Results

Table 3 presents the meta-analytic estimates and I2 with confidence intervals for each of the 37 included effects. 

##Table 3.
```{r heterogeneity-table}
#library(kableExtra)

het %>% knitr::kable("latex", align = "c", booktabs = TRUE,digits = 2, col.names = c("RS", "Effect", "k", "Effect type", "Estimate", "I2(%)", "ci.lb", "ci.ub")) %>% 
  kable_styling("scale_down") %>%
  footnote(threeparttable = TRUE, escape = FALSE,
           general = " Effects were estimated in metafor using REML. The following effects are odds ratios transformed into standardized mean differences: 'Allowed vs. forbidden', 'Gain vs. loss framing', 'Norm of reciprocity', 'Low vs. high category scales'. RS = Replication Study, k = no. primary studies, Estimate = Point estimates of effect sizes, ci.lb = lower bound of I2 95\\\\% confidence interval, ci.ub = upper bound of I2 95\\\\% confidence interval. SMD = Standardized Mean difference (Hedge's g), MD = Mean Difference, RD = Risk Difference, r = correlation. (OSF.io/XXXX)")


```


We examined the evidence for heterogeneity across a set of pre-registered multi-lab replication projects. Few effects show evidence of heterogeneity. The lower bound I2 confidence interval excludes zero for only 7/37 effects (19%; Table 3), all part of the ML1 project. The percentage of heterogeneity estimates larger than 0 (23/37; 62%) suggests heterogeneity for at least some effects, as this percentage is higher than the expected frequency of non-zero estimates under homogeneity (46%, or about 17/37), based on the chi-square distribution and average k across projects. Of those effects whose lower bound confidence interval includes zero, 15/30 (50%) have confidence intervals that exceed 50% (medium heterogeneity). 

[add segment on power with figures when ready]

Larger effect sizes appear to be associated with higher heterogeneity estimates. In Table 3, the four effects with highest I2 estimates also have the largest effect sizes. As expected, heterogeneity tends to increase with (absolute) effect size, with effect size sufficient to explain 61% of variation in I2 estimates according to a simple regression model (Figure 1). The closely related, but unbounded, heterogeneity estimate H2 provides a similar result (R2 = .53). Excluding Anchoring effects as robustness check makes little difference (R2 = .51 for I2). 

##Figure 1
```{r}
knitr::include_graphics("../code/effect_het.png")
```

##Discussion

There is little evidence for heterogeneity amongst current pre-registered multi-lab replication projects in psychology. Across 10 projects, only 7/37 (19%) of primary outcomes had confidence intervals that excluded zero heterogeneity. Moreover, while each replication of an effect only varied on two contextual factors (sampling population and settings), the effects that showed evidence for heterogeneity were primarily effects that might have been expected to be sensitive to the changes in sampling population. Save two effects (Anchoring - Everest and IAT correlation math), all other effects that demonstrated heterogeneity were related to the US. They either asked questions about the US (anchoring effects), persons related to the US (Quote attribution) or issues that are well-known to generate strong debate in the US (i.e., free speech; allowed vs. forbidden). Although ML1 tested US vs. non-US as a moderator of heterogeneity and found very small effect sizes, these are all effects for which heterogenous responses also within the US would be unsurprising (e.g., someone living close to Chicago is more likely to know the population of Chicago). Our results suggest little reason to believe heterogeneity is the norm in psychology, and thus imply that citing heterogeneity as a reason for non-replication of an effect is unwarranted unless predicted _a priori_. 

Complicating the discussion of heterogeneity is that there amongst the studied effects was a clear association between effect size and heterogeneity. There are thus both good theoretical (see introduction) and empirical reasons to expect larger effect sizes to exhibit comparatively more heterogeneity when using observed effect sizes in a meta-analysis. Since not all measurements (e.g., behavioral) admit adjusting for reliability as easily as questionnaires, a potential correlation between effect size and heterogeneity could be difficult to control for. However, the relatively small number of effects in our sample means this association might be an artifact of the data. 

There are some additional limits to the generalizability of claims based on the data in our study. For one, the included effects are neither a representative nor random sample of effects in psychology and as such do not support making strong claims about average heterogeneity levels in psychology. In respect to this point we look forward to the forthcoming Many Labs 2 project which should provide considerable new data on the prevalence of heterogeneity in psychology. In addition, while both the I2 confidence intervals and distribution of point estimates support a small number of heterogenous effects amongst those studied, most confidence intervals are congruent with anything from low to high heterogeneity as defined by [Higgins et al]. As always, absence of evidence is not evidence of absence. Finally, since the included studies only vary on two contextual factors (sample population and settings) , the data tell us nothing about the included effects' susceptibility to changes in the other two factors (treatment and measurement variables).

To conclude, evidence does not appear to support the belief that heterogeneity is the norm in psychology. Large-scale reviews of meta-analyses in psychology suggest considerable heterogeneity across the field of psychology [ref: Stanley; van Erp et al]. However, in our experience psychologist tend to include widely varying studies in their meta-analyses, sometimes varying on all four contextual factors (or more). This is perhaps unsurprising considering the lack of theory delineating boundary conditions in psychology [ref: Simons et al].  Our sample, despite consisting of the arguably best, least biased, meta-analytic data currently available in psychology, was unfortunately relatively small and not representative of the many varied effects studied in psychology. As such we are enthusiastic about the many multi-lab initiatives in psychology (Registered Replication Reports, Many Labs 2, ManyBabies, the Psych Science Accelerator) which in the future will provide considerable new data and enable researchers to re-evaluate the prevalence of heterogeneity in psychology. At current there is little evidence to support the notion that heterogeneity is widespread in psychology. 

##References