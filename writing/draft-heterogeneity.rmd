---
title: "Limited evidence for widespread heterogeneity in psychology"
author: "Anton Ohlsson Collentine"
header-includes:
   - \setlength\parindent{24pt}
   - \usepackage{setspace}
   - \doublespacing
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r load data and packages}
library(readr)
library(dplyr)
library(kableExtra)
library(metafor)
library(purrr)

dat <- read_csv("../data/collated_summary_data.csv")
```


```{r prep-heterogeneity-table}
#library(metafor)
#library(dplyr)
#library(purrr)

est_heterogen_smd_raw <- function(x){
  if(any(x[, "effect_type"] == "Risk difference")){ #without the 'any' we will get warnings because we apply 'if' to a vector
    
    fit <- rma(measure = "RD", ai = outcome_t1, bi = outcome_t2, ci = outcome_c1, di = outcome_c2,  n1i = ntreatment, n2i = ncontrol,  data = x)
    
  } else if(any(x[, "outcomes1_2"] == "mean _ SE")){  
    
    fit <-  rma(yi = effect_size, sei = outcome_c2,  data = x) 
  
  } else if(any(x[, "effect_type"] == "Raw mean difference")){
    if(any(x[, "rs"] %in% c("RRR5", "RRR7"))){ #For these us the Knapp and Hartung adjustment of standard errors
      
      fit <- rma(measure = "MD", m1i = outcome_t1, m2i = outcome_c1, sd1i = outcome_t2, sd2i = outcome_c2, n1i = ntreatment, n2i = ncontrol, test ="knha", data = x)
    } else{
      fit <- rma(measure = "MD", m1i = outcome_t1, m2i = outcome_c1, sd1i = outcome_t2, sd2i = outcome_c2, n1i = ntreatment, n2i = ncontrol, data = x)
    }
  
  } else if(any(x[, "outcomes1_2"] == "mean _ SD")){  
    
    fit <- rma(measure = "SMD", m1i = outcome_t1, m2i = outcome_c1, sd1i = outcome_t2, sd2i = outcome_c2, n1i = ntreatment, n2i = ncontrol, data = x) 
       
  } else if(any(x[, "effect_type"] == "r")){
    if(any(x[, "rs"] == "ML1")){
      fit <- rma(measure = "COR", ri = effect_size, ni = Ntotal,  data = x)
    } else{ #for ML3
      fit <- rma(measure = "UCOR", ri = effect_size, ni = Ntotal,  data = x, vtype = "UB")
      #Note that ML3 specifies measure = "COR" with vtype = "UB", but that this is specified as above in the latest metafor 
    }

  } else{ #for all two-group count effects
    
    fit <- rma(measure = "OR2DL", ai = outcome_t1, bi = outcome_t2, ci = outcome_c1, di = outcome_c2, 
               n1i = ntreatment, n2i = ncontrol,  data = x)
    #standardized, ML1 used 'OR2D' which = 'OR2DL' in latest version of metafor
    
  }
  
  I2_1 <- confint(fit)$random[3, ] #Gives us the I2 estimate and its confidence interval
  data.frame(eff_size = fit$b[[1]], #effect size (point estimate)
             s_I2 = I2_1[1], s_ci.lb = I2_1[2], s_ci.ub = I2_1[3]) #I2 + CI
}

#Apply function to data
res <- dat %>% 
  split(.$effect) %>%  #separate by effect, necessary step otherwise the function is applied overall
  map_dfr(est_heterogen_smd_raw, .id = "effect") #apply function, rbind results into dataframe ('dfr'), make sure purrr is up to date

OR <- c("Allowed vs. forbidden", "Gain vs. loss framing", "Norm of reciprocity", "Low vs. high category scales") #Effects reported as d in many labs 1 but meta-analyzed as OR

#Add rs to the data-frame
het <- dat %>% 
  select(rs, Site, effect, effect_type) %>% 
  group_by(rs, effect) %>% 
  summarize(effect_type = unique(effect_type),
            k = n_distinct(Site)) %>% 
  left_join(res) %>% 
  mutate(effect_type = recode(effect_type, 
                              d = "SMD.",
                              'Raw mean difference' = "MD",
                              'Risk difference' = "RD")) %>% 
  select(rs, effect,k , everything()) %>% 
  arrange(desc(s_I2),  desc(s_ci.ub)) #sort by I2 and upper CI bound
```
Empirical research typically proceeds in two stages. First, belief in the existence of an effect is established. Second, its generalizability is examined by exploring its boundary conditions. In the first stage, we use inferential statistics  to minimize the risk that a discovery is due to sampling error.  In the second stage, we ask to what extent the effect depends on a particular choice of four contextual factors; the 1) sample population, 2) settings, 3) treatment variables and 4) measurement variables [ref: Campbell & Stanley]. This extent is often explored through replications of the original study with some variation, and once sufficient studies have accumulated through meta-analysis. In meta-analysis, the heterogeneity of an effect size (henceforth referred to as heterogeneity) is a measure of an effect's susceptibility to changes in these four factors. An effect strongly dependent on one or more of the four factors, unless controlled for, should exhibit high heterogeneity. In this paper we examine what degree of heterogeneity can be expected in replication studies in psychology and explore a proposed relationship with effect size.

Heterogeneity is of concern for several reasons. First and foremost, unaccounted for heterogeneity can have unacceptable consequences. This may be most readily evident for medicine, where in the case of heterogeneity an intervention that is successful for some may have direct negative health consequences for others. However, the adoption of psychological research for business, law enforcement, and policymaking in general (e.g., the widespread use of 'nudges'; https://www.knowablemagazine.org/article/society/2018/nudging-grows-and-now-has-government-job) means its impact can be widespread and the consequences of heterogeneity should be no less a concern for psychologists. Second, unaccounted for heterogeneity is an indication of incomplete theory, since it suggests that a theory is unable to predict all contextual factors of importance to its claims. While heterogeneity reveals that a theory has flaws, by itself it does not damn a theory, and can be seen as an opportunity for theoretical advancement [ref: Tacket et al]. Third, heterogeneity can create controversy in the interpretation of replication results. The lack of a clear line for the degree of heterogeneity (susceptibility to contextual factors) that would make an effect uninteresting often results in heated discussions between original and replication authors. In response to this common area of disagreement [ref Simons et al] recently proposed that authors include a statement to explicitly delineate the level of heterogeneity that would make an effect lose its value. Fourth, meta-analytic techniques that attempt to correct for publication bias in their estimate tend to fail in the presence of heterogeneity [refs: Inzlicht; McShane]. This is problematic considering widespread publication bias [refs] and its impact on meta-analytic estimates. Finally, heterogeneity affects the interpretation of meta-analytic estimates as either _the_ true effect size (under homogeneity) or the average of the true effect sizes (under heterogeneity).

It is a common belief that heterogeneity is the norm in psychology. For example, several authors [ref: Tacket et al, McShane et al] argue that if we were to expect heterogeneity to be absent or minimal anywhere, it would be in pre-registered multi-lab projects with a common protocol, and that heterogeneity has been reported even under such circumstances [e.g., ref: Many Labs 1] is an indication of widespread heterogeneity in psychology. In addition, recent large scale reviews of meta-analyses in psychology [ref: Stanley; van Erp et al] report that much variability in effect sizes can be attributed to heterogeneity, with median estimates of 74% and 70.62% respectively [ref: Stanley]. In comparison, the median heterogeneity estimate in medicine is considerably lower: 21% amongst Cochrane meta-analyses [ref: Ioannidis  "uncertainty in heterogeneity"]. 

Heterogeneity is often measured by the I2 index [ref: Higgins & Higgins], which allows comparison of estimates across meta-analyses and has an intuitive interpretation. It can be interpreted as the percentage of variability in effect sizes that is due to heterogeneity amongst the true effects (that is, dependency on contextual factors) rather than sampling variance, and ranges from 0-100%. The I2 index is a transformation of the Q-test [ref: Cochran] by taking $[Q - (k - 1)] / Q \times 100$%, where k is the number of studies included in the meta-analysis and a negative I2 is set to zero.  

Heterogeneity (I2) estimates typically contain considerable uncertainty. Both the Q-test and I2 have low power [ref: Jackson; Huedo-Medina]. This complicates the discussion of heterogeneity, because while I2 always provides an estimate of heterogeneity, this estimate is often surrounded by wide confidence intervals [ref: Ioannidis  "uncertainty in heterogeneity"]. For example, Ioannidis reports that in a large set of Cochrane meta-analyses, all meta-analyses with I2 point estimates of 0% had upper 95% confidence intervals that exceeded 33%. In addition, under homogeneity I2 has a central chisquare distribution [ref: von Hippel], a distribution that is right-skewed with more than 40% of observations falling above the expected value in almost all cases (for all k > 4). In other words, even in the absence of heterogeneity, a meta-analysis of 5 or more studies will have an I2 point estimate above zero in more than 40% of cases. Point estimates (expected values) of heterogeneity may thus be congruent with a wide range of true heterogeneity levels. Despite exhortations to the contrary [ref: Ioannidis "uncertainty in heterogeneity"] , it remains common to omit confidence intervals in the reporting of I2 [e.g., ML1 and ML3]. 

Effect size influences heterogeneity. Intuitively, it makes sense to think that if there is no meta-analytic effect there is nothing to moderate (i.e., no heterogeneity). However, a null or near null effect size estimate may arise from failure to consider contextual factors ('hidden moderators', [ref: van Bavel]) and does not by itself imply the absence of heterogeneity. This is especially true when the uncertainty in I2 estimates (see above) is taken into account. A large effect size on the other hand, can be expected to lead to more heterogeneity. To illustrate, consider a meta-analysis of say, the correlation between neuroticism and procrastination. Each included study would need to measure the two variables somehow, possibly the same way across studies. However, because of individual differences, measurement is not equally exact for everyone, and between studies the measurement reliabilities will differ either due to sampling variance (that the sample happens to be more or less homogenous) or to differences in contextual factors (e.g., sampling population, method of measurement). This means that even if the underlying effect size is the same, the observed correlation between the two variables will differ between studies. If measurement reliabilities stay the same, these differences will increase with the underlying effect, resulting in more variability being ascribed to heterogeneity. More formally, an observed correlation $r_{xy}$ is equivalent to the true correlation $\rho_{xy}$ multiplied by the square root of the measurement reliabilities for X ($R_{xx'}$) and Y ($R_{yy'}$): $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$. As such, if $\sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$ stays constant while $\rho_{xy}$ increases the observed differences between studies will increase (see Table 2). 

##Table 1.
###Heterogeneity may increase with effect size
```{r effect-size-heterogeneity-table}
#library(kableExtra)
rooted_relabilities <- matrix(rep(c(.8, .7,.6), 3), ncol = 3)
rho <- c(0, .5, .8)
rel <- c("R = .8", "R = .7", "R = .6")

rooted_relabilities <- sweep(rooted_relabilities, 2, rho, "*")
diff_row <- data.frame(rel = "Diff.", X1 = 0, X2 = 0.05, X3 = 0.08)

het_demo <- data.frame(rel, rooted_relabilities)
het_demo <- rbind(het_demo, diff_row)

het_demo %>% knitr::kable("latex", booktabs = T, col.names = c("", "Rho = 0", "Rho = .5", "Rho = .8")) %>% 
  kable_styling() %>% 
  footnote(threeparttable = TRUE,
           general = "R = square root of the multiplied reliabilities") %>% 
  row_spec(3, hline_after = T)

```
We examine heterogeneity across 10 pre-registered multi-lab replication projects in psychology (Table 2). In light of the uncertainty surrounding I2-estimates we explore what conclusions can be drawn about heterogeneity in psychology from these projects, as well as the relationship between effect size and heterogeneity. These 10 projects examined a total of 37 primary outcome variables and arguably represent the best, least biased, meta-analytic data currently available in psychology. All included studies vary on a minimal number of contextual factors [as recognized by e.g., Tacket et al]: only sample population and settings. Nonetheless, contrary to the argument of [Tacket et al], if an effect is sensitive to changes in these factors we may still expect to find some heterogeneity. On the other hand, heterogeneity estimates will only inform us of susceptibility to changes in these two factors, with no information on susceptibility to changes in other contextual factors.

##Table 2.
```{r data-summary-table}
#library(dplyr)
#library(kableExtra)

#Shortened version of APA reference
papers <- c('Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Jr., Bahník, S., Bernstein, M. J., . . . Nosek, B. A. (2014). Investigating variation in replicability: A "many labs" replication project.', 
'Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., ... & Brown, E. R. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication.',
'Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., ... & Brown, E. R. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication.',
'Alogna, V. K., Attaya, M. K., Aucoin, P., Bahník, S., Birch, S., Birt, A. R., ... & Buswell, K. (2014). Registered replication report: Schooler and engstler-schooler (1990).',
'Eerland, A., Sherrill, A. M., Magliano, J. P., Zwaan, R. A., Arnal, J. D., Aucoin, P., ... & Crocker, C. (2016). Registered replication report: Hart & Albarracín (2011).',
'Hagger, M. S., Chatzisarantis, N. L., Alberts, H., Anggono, C. O., Batailler, C., Birt, A. R., ... & Calvillo, D. P. (2016). A multilab preregistered replication of the ego-depletion effect.',
'Cheung, I., Campbell, L., LeBel, E. P., Ackerman, R. A., Aykutoglu, B., Bahník, S., ... & Carcedo, R. J. (2016). Registered Replication Report: Study 1 from Finkel, Rusbult, Kumashiro, & Hannon (2002).',
'Wagenmakers, E. J., Beek, T., Dijkhoff, L., Gronau, Q. F., Acosta, A., Adams Jr, R. B., ... & Bulnes, L. C. (2016). Registered Replication Report: Strack, Martin, & Stepper (1988).',
'Bouwmeester, S., Verkoeijen, P. P., Aczel, B., Barbosa, F., Bègue, L., Brañas-Garza, P., ... & Evans, A. M. (2017). Registered Replication Report: Rand, Greene, and Nowak (2012).',
"O'Donnell, M., Nelson, L., McLatchie, N. M., & Lynott, D. J. (2017). Registered Replication Report: Dijksterhuis & van Knippenberg (1998)")

summary.table <- dat %>% 
  group_by(rs) %>% 
  summarize(k = n_distinct(Site),
            Countries = n_distinct(country), 
            Effects = n_distinct(effect),
            Participants = if(Effects == 1){sum(Ntotal)}
                           else{sum(Ntotal) / Effects}) %>% #average across effects
  mutate(Paper = papers) %>% 
  select(RS = rs, Paper, everything()) #change order of columns

summary.table %>% knitr::kable("latex", booktabs = T, digits = 0) %>% 
  kable_styling() %>% 
  column_spec(2, width = "25em") %>% #specify width of paper column
  footnote(threeparttable = TRUE,
           general = "For studies with several effects the number of participants is the average across effects, rounded to the closest whole number. Participant numbers are those used for primary analyses by original authors (i.e., after exclusions). RS = Replication Study, k = no. primary studies, ML = Many Labs, RRR = Registered Replication Report. (OSF.io/XXXX)")
```

###Method

All code and data for this project are available on the Open Science Framework (OSF) at osf.io/XXXX. We refer to relevant files on OSF using brackets and links in the sections below, that is (osf.io/XXXX/). We ran all analyses using R version X [ref: R citation].

We downloaded and collated summary data from 10 pre-registered multi-lab replication projects in psychology (Table 2). Data from all 10 projects were publicly available on the Open Science Framework (osf.io) and downloaded between 2018/02/01 and 2018/03/31. Although some projects (e.g. RRR4) reported results from several outcome variables, we only included primary outcome variables as stated in accompanying publications, resulting in a total of 37 effects. For each effect we extracted (osf.io/XXXX/) summary data (e.g., means and standard deviations) at the level of the lab as specified by the original authors for their primary analysis (i.e., typically after exclusions). We extracted information on the country of each lab, whether participants were physically in the lab for the study, total number of participants per lab, type of effect size, and additional information related to each effect (see codebook; osf.io/XXXX/). Extracted data were in a variety of formats: Excel (Many labs 1, RRR1 & RRR2), CSV (Many labs 3, RRR3, RRR4, RRR5, RRR6) and as PDF tables (RRR7, RRR8). In two cases (RRR5 and RRR6) it was necessary to download the raw data to extract summary data. Although a particular lab may have participated in several projects, the lab indicator was typically not the same across projects. Even so, we kept the original lab indicators to facilitate comparing observations in our dataset with the original datasets. Finally, we collated he summary data for all effects into one dataset for analysis (osf.io/XXXX/). 

To examine heterogeneity across effects, we computed meta-analytic estimates for all 37 effects in our dataset (Table 3). We ran all analyses as specified by the replication authors (osf.io/XXXX/). All effects were estimated with random-effects models and the Restricted Maximum Likelihood (REML) estimator using the R-package metafor [ref], though with a variety of outcome variables: correlations, standardized mean differences, raw mean differences, odds ratios and risk differences. ML1 transformed odd ratios into standardized mean differences when meta-analyzing under the assumption that responses followed logistic distributions [ref: Sanchez-Meca; metafor]. Two projects (RRR5 and RRR7) used the Knapp and Hartung adjustment of the standard errors [ref: Knapp & Hartung] and ML3 correlations were corrected for bias [ref: Hedges 1989; metafor]. After estimating effects, Ml3 transformed correlations into eta-squared for reporting, which we did not. For each effect we estimated I2 and its 95% confidence interval. Confidence intervals were calculated in metafor which uses the Q-profile method [ref Jackson 2014].

To examine the relationship between effect size and I2 we converted all effect sizes to a common metric (osf.io/XXXX/). Since I2 is logically and mathematically bounded at zero, we also correlated effect sizes with the closely related heterogeneity estimate H2, which is calculated as $H2 = Q / (k - 1)$. We first converted all effects into correlations (r) and used the R-package metafor [ref] to estimate I2 and meta-analytic effects expressed as Fisher's z. In doing so, we fit random-effects models with metafor's default REML estimator. Fisher's z is a normalizing correlation transformation that ranges from negative to positive infinity, though except for extreme correlations it stays close to the -1 to 1 range [ref]. For mean differences we calculated the pooled standard deviation, [ref Borenstein in Cooper & Hedges, 2009, p. 226), standardized the effect size and converted it to a correlation (p. 234) with a correction factor for unequal sample sizes (p. 234). In one case (RRR8), we first had to convert reported standard errors into pooled standard deviation (p.224). For risk differences and odds ratios we first added 1/2 to a cell if it was empty to avoid dividing by zero, next calculated the logarithmic odds ratio (p. 266), converted this to Cohen's d (p. 232) and then finally to a correlation. All formulas used are presented in Appendix A. 

##Results

Table 3 presents the meta-analytic estimates and I2 with confidence intervals for each of the 37 included effects. 

##Table 3.
```{r heterogeneity-table}
#library(kableExtra)

het %>% knitr::kable("latex", align = "c", booktabs = TRUE,digits = 2, col.names = c("RS", "Effect", "k", "Effect type", "Estimate", "I2(%)", "ci.lb", "ci.ub")) %>% 
  kable_styling("scale_down") %>%
  footnote(threeparttable = TRUE, escape = FALSE,
           general = " Effects were estimated in metafor using REML. The following effects are odds ratios transformed into standardized mean differences: 'Allowed vs. forbidden', 'Gain vs. loss framing', 'Norm of reciprocity', 'Low vs. high category scales'. RS = Replication Study, k = no. primary studies, Estimate = Point estimates of effect sizes, ci.lb = lower bound of I2 95\\\\% confidence interval, ci.ub = upper bound of I2 95\\\\% confidence interval. SMD = Standardized Mean difference (Hedge's g), MD = Mean Difference, RD = Risk Difference, r = correlation. (OSF.io/XXXX)")


```


Few effects show evidence of heterogeneity. The lower bound I2 confidence interval excludes zero for only 7/37 effects. Heterogeneity in a small number of effects is supported by the distribution of I2 point estimates. In a complete absence of heterogeneity, we would expect approximately 46% of point estimates across all effects to be non-zero, based on the chisquare distribution and average k across projects. That is, for about 17/37 effects. Table 3 shows that 23/37 effects have non-zero point estimates, indicating a small presence of heterogeneity. 

Larger effect sizes appear to lead to higher heterogeneity estimates. In Table 3, the four effects with highest I2 estimates also have the largest effect sizes. As expected, heterogeneity tends to increase with (absolute) effect size, with effect size sufficient to explain 61% of variation in I2 estimates according to a simple regression model (Figure 1). The closely related, but unbounded, heterogeneity estimate H2 provides a similar result (R2 = .53).

##Figure 1
```{r}
knitr::include_graphics("../code/effect_het.png")
```

##Discussion

There is little evidence of heterogeneity amongst current pre-registered multi-lab replication projects. Across 10 projects, only 7/37 (19%) of primary outcomes showed evidence for heterogeneity. Moreover, while each replication of an effect only varied on two contextual factors (sampling population and settings), the effects that showed evidence for heterogeneity were primarily effects that might have been expected to be sensitive to the changes in sampling population. Save two effects (Anchoring - Everest and IAT correlation math), all other effects that demonstrated heterogeneity were related to the US. They either asked questions about the US (anchoring effects), persons related to the US (Quote attribution) or issues where the US is well-known to be exceptional (i.e., free speech; allowed vs. forbidden). Considering all seven effects belonged to the ML1 replication project, which collected data from labs in 10 different countries, these effects should have been expected to exhibit heterogeneity. [note to Jelte and Marcel: I am not sure this argumentation holds up, since ML1 did test US vs. non-US as a moderator and found very small effect sizes]

Complicating the discussion of heterogeneity is that there amongst the studied effects was a clear association between effect size and heterogeneity. There are thus both good theoretical (see introduction) and empirical reasons to expect larger effect sizes to exhibit comparatively more heterogeneity when using observed effect sizes in a meta-analysis. Since not all measurements (e.g., behavioral) admit adjusting for reliability as easily as questionnaires, this can be difficult to control for.

There are some limits to the generalizability of claims based on the data in our study. For one, the included effects are neither a representative nor random sample of effects in psychology and as such do not support making strong claims about average heterogeneity levels in psychology. In respect to this point we look forward to the forthcoming Many Labs 2 project which should provide considerable new data on the prevalence of heterogeneity in psychology. In addition, while both the I2 confidence intervals and distribution of point estimates support a small number of heterogenous effects amongst those studied, most confidence intervals are congruent with anything from low to high heterogeneity as defined by [Higgins et al]. As always, absence of evidence is not evidence of absence. Finally, since the included studies only vary on two contextual factors, the data tell us nothing about the included effects susceptibility to changes in the other two factors (treatment and measurement variables).

Evidence does not appear to support the belief that heterogeneity is the norm in psychology. Large-scale reviews of meta-analyses in psychology suggest considerable heterogeneity across the field of psychology [ref: Stanley; van Erp et al]. However, in our experience psychologist tend to include widely varying studies in their meta-analyses, sometimes varying on all four contextual factors (or more). This is perhaps unsurprising considering the lack of delineating theory in psychology [ref: Simons et al]. In support of this notion, [Ioannidis; uncertainty in heterogeneity] reports that, even after excluding meta-analyses of 3 or fewer studies, the median number of studies included in Cochrane meta-analyses is only 7, whereas in the dataset of [ref: van Erp et al] containing 747 meta-analyses in psychology, the median number of studies (effects) per meta-analysis is 12.]. Our examination of heterogeneity in pre-registered multi-lab replication projects, arguably the least biased meta-analytic data currently available in psychology, indicates that there is little evidence to support the notion that heterogeneity is widespread in psychology. 

##References