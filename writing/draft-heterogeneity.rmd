---
title: "Insufficient evidence for heterogeneity in psychology"
author: "Anton Ohlsson Collentine"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r load data and packages}
library(readr)
library(dplyr)
library(kableExtra)

dat <- read_csv("../data/collated_summary_data.csv")
```

In most research there is a natural pattern of studies. We first establish belief in the existence of an effect and then wish to know how well it generalizes by exploring its boundary conditions. The first we do by using inferential statistics to minimize the risk that a discovery is due to sampling error. In the second, we are asking to what extent the effect depends on a particular choice of contextual factors such as 1) sample population, 2) settings, 3) treatment variables and 4) measurement variables [ref: Campbell & Stanley]. This is often explored through replications of the original study with some slight variation, and once sufficient studies have accumulated through meta-analysis. In meta-analysis, the estimated heterogeneity of an effect size (henceforth heterogeneity) is a measure of an effect's susceptibility to changes in these four factors. An effect strongly dependent on one or more of the four factors should exhibit high heterogeneity unless controlled for. 

Heterogeneity is of concern for several reasons. First, it affects the interpretation of the meta-analytic estimate as either _the_ true effect size (homogeneity) or the average of the true effect sizes (heterogeneity). Second, if boundary conditions are unknown, it makes it more difficult to 'successfully' replicate an effect. In the presence of heterogeneity even a well-powered single study may capture the average true effect in less than 50% of cases [ref: Voelkl]. Third, meta-analytic techniques that attempt to correct for publication bias in their estimate tend to fail in the presence of heterogeneity [refs: Inzlicht, McShane]. 

Several recent papers have argued that heterogeneity is the norm in psychology [ref: Tacket et al, McShane et al]. These authors argue that, if anywhere, heterogeneity should be minimal in pre-registered multi-lab studies with a common protocol, and the fact that heterogeneity has been found even under such circumstances [e.g., ref: Many Labs 1] is an indication that heterogeneity is widespread in psychology. In addition, recent large scale reviews of meta-analyses in psychology [ref: Stanley; van Erp et al] report that much variability in effect sizes can be attributed to heterogeneity, with median estimates of 74% and 70.62% respectively [ref: Stanley].

We caution against broad claims of particular heterogeneity levels in psychology. We have multiple concerns. We think that the role of effect sizes is both underestimated (when large) and overestimated (when small) when considering heterogeneity. In addition, we think that too much attention is paid to point estimates in view of the uncertainty conferred by accompanying confidence intervals. We choose to dwell less on this latter point as it has already been well-made previously [ref: Ioannidis]. Generally, we argue that the current evidence is insufficient to make general claims about the prevalence of heterogeneity in psychology.

Larger effect sizes lead to more heterogeneity. In classical test theory, an observed correlation $r_{xy}$ is equivalent to the true correlation $\rho_{xy}$ multiplied by the square root of the measurement reliabilities for X ($R_{xx'}$) and Y ($R_{yy'}$): $r_{xy} = \rho_{xy} \times \sqrt{R_{xx'}} \times \sqrt{R_{yy'}}$. Across different studies of the same effect the measurement reliabilities will differ due either to sampling variance (that the sample happens to be more or less homogenous) or to small (or large) differences in contextual factors (e.g., location). Given that the reliabilities will differ between studies, so will the observed effect sizes even when the underlying effect is the same. It follows from the above presented equation that the larger the underlying effect size, the larger the differences between observed effect sizes in studies with differing reliabilities. Hence the estimated heterogeneity will increase with effect size. 

A null meta-analytic effect size does not imply absence of heterogeneity. Intuitively, it makes sense to think that if there is no effect there is nothing to moderate (i.e., no heterogeneity). However, while this might often be true it is by no means a truism. A null or near null effect size estimate may arise from failure to consider contextual factors ('hidden moderators', [ref: van Bavel]) and does not by itself imply the absence of heterogeneity. This is especially true when also taking into consideration heterogeneity confidence intervals.

In the following sections we clarify and illustrate our concerns using empirical data and the I2 index as a measure of heterogeneity [ref: Higgins & Higgins]. We make use of summary data from 10 pre-registered multi-lab replications that examined a total of 37 psychological effects (see Table 1). These data arguably represents the best, least biased, meta-analytic data available in psychology. The I2 index is an intuitive and much used measure of heterogeneity which ranges from 0% to 100%. It can be interpreted as the percentage of variability in effect sizes that is due to heterogeneity amongst the true effects rather than sampling variance and is a transformation of the Q-test [ref: Cochran] by taking $[Q - (k - 1)] / Q \times 100$%, where k is the number of studies included in the meta-analysis. I2 is a relative measure of heterogeneity that allows comparison of estimates across meta-analyses.

##Table 1
```{r data-summary-table}
#library(dplyr)
#library(kableExtra)

summary.table <- dat %>% 
  group_by(rs) %>% 
  summarize(Labs = n_distinct(Site),
            Countries = n_distinct(country), 
            Effects = n_distinct(effect),
            Participants = if(Effects == 1){sum(Ntotal)}
                           else{sum(Ntotal) / Effects}) %>% #average across effects
  rename(Study = rs)

summary.table %>% knitr::kable("latex", booktabs = T, digits = 0) %>% 
  kable_styling(position = "left") %>% 
  footnote(threeparttable = TRUE,
           general = "For studies with several effects the number of participants is the average across effects, rounded to the closest whole number. Participant numbers are those used for primary analyses by original authors (i.e., after exclusions). ML = Many Labs, RRR = Registered Replication Report.")
```

<!-- General thought: Ml1 compares heterogeneity from the perspective of between lab vs. between effects and concludes that heterogeneity depends mostly on the effect. However, it seems unsurprising that a certain lab (when not perversely motivated) does not systematically bias effects, but rather that a location might have differential effects on different studied effects. -->

<!-- This might mean that we have missed an important moderator, or that there is a lot of small moderators missed (thinking about it, this seems to differ from random fluctuations in that they are still systematic in some sense, on the other hand, whether they cancel out or not is in itself due to "chance". If an effect (but perhaps not others) is moderated by what time of the day the study is run this can lead to heterogeneity, but say there is also a time-independent moderator such as whether the room is warm -> then we get heterogeneity even if two labs measure at the same time, at what point do these things become "noise" rather than systematic bias? Because we have so few labs it is difficult to separate these?) -->

